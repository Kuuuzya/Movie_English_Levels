{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7b4703",
   "metadata": {},
   "source": [
    "<a name='introduction'></a>\n",
    "# Проект \"Определение уровня английского для фильмов\"\n",
    "\n",
    "В данном проекте необходимо определить, какой уровень английского языка представлен в фильме. Уровень английского языка определяется в соответствии с уровнем Oxford CEFR. Определение уровня осуществляется на основе субтитров к фильму. В дополнение, можно добавлять фильмы и субтитры к ним из открытых источников, поскольку первоначально в дасете 241 фильм.\n",
    "\n",
    "**План работы:**\n",
    "1. [Предобработка данных](#data_preprocessing)\n",
    "2. [Выбор метрики](#metric)\n",
    "3. [Создание модели](#model)\n",
    "4. [Анализ результатов](#analysis)\n",
    "5. [Сохранение модели](#model_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e26840",
   "metadata": {},
   "source": [
    "<a name='data_preprocessing'></a>\n",
    "## 1. Предобработка данных\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed5c45",
   "metadata": {},
   "source": [
    "### 1. 1. Общий файл с фильмами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67aa856e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:57:40.570555Z",
     "start_time": "2023-05-14T13:57:40.560846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Импортирование необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import pysrt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kuuuzya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kuuuzya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kuuuzya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kuuuzya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/kuuuzya/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/kuuuzya/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T13:57:46.577742Z",
     "start_time": "2023-05-14T13:57:46.570087Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e97b8c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:53:21.863879Z",
     "start_time": "2023-05-14T13:53:21.767149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер: (241, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                              movie   level\n0         10_Cloverfield_lane(2016)      B1\n1  10_things_I_hate_about_you(1999)      B1\n2              A_knights_tale(2001)      B2\n3              A_star_is_born(2018)      B2\n4                     Aladdin(1992)  A2/A2+",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10_Cloverfield_lane(2016)</td>\n      <td>B1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10_things_I_hate_about_you(1999)</td>\n      <td>B1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A_knights_tale(2001)</td>\n      <td>B2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A_star_is_born(2018)</td>\n      <td>B2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Aladdin(1992)</td>\n      <td>A2/A2+</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импорт файла\n",
    "df_movies = pd.read_excel(f'Source/English_scores/movies_labels.xlsx')\n",
    "\n",
    "df_movies.drop('id', axis=1, inplace=True)\n",
    "df_movies.columns = ['movie', 'level']\n",
    "\n",
    "print('Размер:', df_movies.shape)\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0e15b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:53:25.317694Z",
     "start_time": "2023-05-14T13:53:25.309248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              movie level  level_num\n0         10_Cloverfield_lane(2016)    B1          2\n1  10_things_I_hate_about_you(1999)    B1          2\n2              A_knights_tale(2001)    B2          3\n3              A_star_is_born(2018)    B2          3\n4                     Aladdin(1992)    A2          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie</th>\n      <th>level</th>\n      <th>level_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10_Cloverfield_lane(2016)</td>\n      <td>B1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10_things_I_hate_about_you(1999)</td>\n      <td>B1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A_knights_tale(2001)</td>\n      <td>B2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A_star_is_born(2018)</td>\n      <td>B2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Aladdin(1992)</td>\n      <td>A2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Замена промежуточный уровень на нижний\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+, B1', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'B1, B2', 'level'] = 'B1'\n",
    "\n",
    "df_movies['level_num'] = df_movies['level'].map({\n",
    "    'A1': 0,\n",
    "    'A2': 1,\n",
    "    'B1': 2,\n",
    "    'B2': 3,\n",
    "    'C1': 4\n",
    "})\n",
    "\n",
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e984c69",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T13:53:27.222456Z",
     "start_time": "2023-05-14T13:53:27.212636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "movie        0\nlevel        0\nlevel_num    0\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пропущенные значения\n",
    "df_movies.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e269ec0a",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-05-14T13:53:30.412276Z",
     "start_time": "2023-05-14T13:53:30.407108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "level\nB2    101\nB1     63\nC1     40\nA2     37\nName: count, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Распределение количества фильмов по уровням \n",
    "df_movies['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430612b7",
   "metadata": {},
   "source": [
    "### 1. 2. Файлы с субтитрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1547cebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:54:49.253726Z",
     "start_time": "2023-05-14T13:54:49.249989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вычленение то, что не относится к словам\n",
    "HTML = r'<.*?>'\n",
    "TAG = r'{.*?}'\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]'\n",
    "UPPER = r'[[A-Za-z ]+[\\:\\]]'\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]'\n",
    "SPACES = r'([ ])\\1+'\n",
    "DOTS = r'[\\.]+'\n",
    "SYMB = r\"[^\\w\\d'\\s]\"\n",
    "\n",
    "\n",
    "# Функция для загрузки и лемматизации субтитров\n",
    "def saving_subs(film_loc):\n",
    "    # Загрузка субтитров по кодированию\n",
    "    film_subs = []\n",
    "    encodings = ['', 'UTF-8-SIG', 'ISO-8859-1', 'utf-8', 'Windows-1252', 'ascii']\n",
    "    encoding_number = 0\n",
    "\n",
    "    # Проверка возможности правильности кодирования, иначе - использование другого\n",
    "    while not film_subs:\n",
    "        try:\n",
    "            film_subs = pysrt.open(\n",
    "                film_loc,\n",
    "                encoding=encodings[encoding_number]\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            encoding_number += 1\n",
    "    \n",
    "    # Удаление лишнего и оставление только слов\n",
    "    text = re.sub(HTML, ' ', film_subs[1:].text)\n",
    "    text = re.sub(TAG, ' ', text)\n",
    "    text = re.sub(COMMENTS, ' ', text)\n",
    "    text = re.sub(UPPER, ' ', text)\n",
    "    text = re.sub(LETTERS, ' ', text)\n",
    "    text = re.sub(DOTS, r'.', text)\n",
    "    text = re.sub(SPACES, r'\\1', text)\n",
    "    text = re.sub(SYMB, '', text)\n",
    "    text = re.sub('www', '', text)\n",
    "    text = text.lstrip()\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = text.lower()\n",
    "\n",
    "    # Лемматизация слов и добавление их в отдельный список\n",
    "    film_words = []\n",
    "    text_list = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "        word = lemmatizer.lemmatize(text_list[i])\n",
    "        # Проверка на наличие слова в списке и отсутствия в списке имен собственных\n",
    "        if word not in film_words:\n",
    "            film_words.append(word)\n",
    "\n",
    "    # Объединение слов в одну строчку\n",
    "    film_words = \" \".join(film_words)\n",
    "    return film_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd9371d5",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-05-14T13:55:04.919887Z",
     "start_time": "2023-05-14T13:54:50.898072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vd/cvcx7y592474yqnsftwldlpm0000gn/T/ipykernel_9329/3942574165.py:33: FutureWarning: Possible nested set at position 1\n",
      "  text = re.sub(UPPER, ' ', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "# Получение субтитров для каждого фильма, воспользовавшись функцией для обработки субтитров\n",
    "for index, row in df_movies.iterrows():\n",
    "    print(index)\n",
    "    try:\n",
    "        subs = saving_subs(f\"Source/English_scores/subtitles_combined/{row['movie']}.srt\")\n",
    "        df_movies.loc[df_movies['movie'] == row['movie'], 'subs'] = subs\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf38462d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:55:07.960365Z",
     "start_time": "2023-05-14T13:55:07.954058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаление фильмов, к которым не были найдены субтитры\n",
    "df_movies = df_movies[(df_movies['subs'] != '') & (df_movies['subs'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1718c",
   "metadata": {},
   "source": [
    "### 1. 3. Слова по уровню языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf23ab",
   "metadata": {},
   "source": [
    "Создадим список со всеми словами, а также отдельный словарь с ключами в виде уровней английского и значениями - индексами начала слов уровня - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da4af671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:55:38.146819Z",
     "start_time": "2023-05-14T13:55:38.137526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ссылки на слова (А1-C1)\n",
    "a1_b2_link = 'Source/Oxford_CEFR_level/The_Oxford_3000_by_CEFR_level.pdf'\n",
    "b2_c1_link = 'Source/Oxford_CEFR_level/The_Oxford_5000_by_CEFR_level.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e18d9b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:55:40.133457Z",
     "start_time": "2023-05-14T13:55:40.127433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Функция для сохранения слов в список из PDF-файла \n",
    "def get_level_words(link):\n",
    "    reader = PyPDF2.PdfReader(link)\n",
    "    \n",
    "    words = ''\n",
    "    for n in range(len(reader.pages)):\n",
    "        words += reader.pages[n].extract_text()\n",
    "        \n",
    "    words = words.split('\\n')\n",
    "    words = [words[n].split()[0] for n in range(len(words)) if n > 1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa66156a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:55:42.809939Z",
     "start_time": "2023-05-14T13:55:42.140594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Сохранение слов по уровням в отдельные переменные\n",
    "a1_b2_words = get_level_words(a1_b2_link)\n",
    "b2_c1_words = get_level_words(b2_c1_link)\n",
    "\n",
    "# Внесем некоторые корректировки\n",
    "a1_b2_words[1] = 'a'\n",
    "a1_b2_words.insert(2, 'an')\n",
    "b2_c1_words.remove('3000')\n",
    "b2_c1_words.remove('B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628ac9b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:55:45.313394Z",
     "start_time": "2023-05-14T13:55:45.307277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['A1',\n 'a',\n 'an',\n 'about',\n 'above',\n 'across',\n 'action',\n 'activity',\n 'actor',\n 'actress']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Объединим слова\n",
    "level_words = a1_b2_words\n",
    "level_words.extend(b2_c1_words)\n",
    "level_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26a62657",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-05-14T13:55:48.998762Z",
     "start_time": "2023-05-14T13:55:48.992414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 0, 'A2': 891, 'B1': 1752, 'B2': 2550, 'C1': 3958}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря с ключами в виде уровня английского и значениями в виде индексов начала слов\n",
    "# данного уровня минус единица\n",
    "levels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "levels_dict = {level: level_words.index(level) for level in levels}\n",
    "\n",
    "print(levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                                movie level  level_num   \n0           10_Cloverfield_lane(2016)    B1          2  \\\n1    10_things_I_hate_about_you(1999)    B1          2   \n2                A_knights_tale(2001)    B2          3   \n3                A_star_is_born(2018)    B2          3   \n4                       Aladdin(1992)    A2          1   \n..                                ...   ...        ...   \n230   Suits.S03E06.720p.HDTV.x264-mSD    C1          4   \n231        Suits.S03E07.HDTV.x264-mSD    C1          4   \n232   Suits.S03E08.480p.HDTV.x264-mSD    C1          4   \n233   Suits.S03E09.480p.HDTV.x264-mSD    C1          4   \n234        Suits.S03E10.HDTV.x264-mSD    C1          4   \n\n                                                  subs  \n0    michelle please don't hang up just talk to me ...  \n1    i'll be right with you so cameron here go nine...  \n2    should we help him he's due in the list two mi...  \n3    get to it black eye open wide it's time testif...  \n4    where the caravan camel roam it's flat and imm...  \n..                                                 ...  \n230  i assume my deal with edward is dead a long yo...  \n231  it's going up on the wall tomorrow and this is...  \n232  darby back me for managing partner i don't wan...  \n233  i'm bonding with your father here speaking of ...  \n234  this is a copy of the letter you wrote to dist...  \n\n[233 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie</th>\n      <th>level</th>\n      <th>level_num</th>\n      <th>subs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10_Cloverfield_lane(2016)</td>\n      <td>B1</td>\n      <td>2</td>\n      <td>michelle please don't hang up just talk to me ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10_things_I_hate_about_you(1999)</td>\n      <td>B1</td>\n      <td>2</td>\n      <td>i'll be right with you so cameron here go nine...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A_knights_tale(2001)</td>\n      <td>B2</td>\n      <td>3</td>\n      <td>should we help him he's due in the list two mi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A_star_is_born(2018)</td>\n      <td>B2</td>\n      <td>3</td>\n      <td>get to it black eye open wide it's time testif...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Aladdin(1992)</td>\n      <td>A2</td>\n      <td>1</td>\n      <td>where the caravan camel roam it's flat and imm...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>Suits.S03E06.720p.HDTV.x264-mSD</td>\n      <td>C1</td>\n      <td>4</td>\n      <td>i assume my deal with edward is dead a long yo...</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>Suits.S03E07.HDTV.x264-mSD</td>\n      <td>C1</td>\n      <td>4</td>\n      <td>it's going up on the wall tomorrow and this is...</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>Suits.S03E08.480p.HDTV.x264-mSD</td>\n      <td>C1</td>\n      <td>4</td>\n      <td>darby back me for managing partner i don't wan...</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>Suits.S03E09.480p.HDTV.x264-mSD</td>\n      <td>C1</td>\n      <td>4</td>\n      <td>i'm bonding with your father here speaking of ...</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>Suits.S03E10.HDTV.x264-mSD</td>\n      <td>C1</td>\n      <td>4</td>\n      <td>this is a copy of the letter you wrote to dist...</td>\n    </tr>\n  </tbody>\n</table>\n<p>233 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T13:56:00.971024Z",
     "start_time": "2023-05-14T13:56:00.967174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ca8fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не улучшает качество моделей!\n",
    "\n",
    "\n",
    "# # Определение количества слов по уровням для каждого фильма\n",
    "# for index, row in df_movies.iterrows():\n",
    "#     # Словарь с подсчетом слов по каждому уровню для отдельного фильма\n",
    "#     film_level_words = {level: [] for level in levels}\n",
    "    \n",
    "#     # Добавление в словарь слов по их уровням\n",
    "#     for word in row['subs'].split():\n",
    "#         try:\n",
    "#             word_index = level_words.index(word.lower())\n",
    "#             if levels_dict['A1'] < word_index < levels_dict['A2']:\n",
    "#                 if word not in film_level_words['A1']:\n",
    "#                     film_level_words['A1'].append(word)\n",
    "#             elif levels_dict['A2'] < word_index < levels_dict['B1']:\n",
    "#                 if word not in film_level_words['A2']:\n",
    "#                     film_level_words['A2'].append(word)\n",
    "#             elif levels_dict['B1'] < word_index < levels_dict['B2']:\n",
    "#                 if word not in film_level_words['B1']:\n",
    "#                     film_level_words['B1'].append(word)\n",
    "#             elif levels_dict['B2'] < word_index < levels_dict['C1']:\n",
    "#                 if word not in film_level_words['B2']:\n",
    "#                     film_level_words['B2'].append(word)\n",
    "#             else:\n",
    "#                 if word not in film_level_words['C1']:\n",
    "#                     film_level_words['C1'].append(word)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "    \n",
    "#     # Подсчет количества слов по уровням\n",
    "#     levels_words_count = {level: len(film_level_words[level]) for level in levels}\n",
    "    \n",
    "#     # Добавление количества слов по уровням в отдельные столбцы\n",
    "#     total_words = sum(levels_words_count.values())\n",
    "#     for level in levels:\n",
    "#         df_movies.loc[df_movies['movie'] == row['movie'], level] = levels_words_count[level] / total_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e668a0",
   "metadata": {},
   "source": [
    "<a name='metric'></a>\n",
    "## 2. Выбор метрики\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c963860",
   "metadata": {},
   "source": [
    "Имеем дело с задачей мультиклассификации. В связи с этим необходимо использовать соответствующие метрики определения качества моделей. Назначение проекта - помочь учащимся, изучающих английский язык. В связи с этим лучше всего использовать F1-меру, поскольку она сочетает в себе и точность (precision), и полноту (recall). Однако, поскольку мы имеем дело с мультиклассификацией, то необходимо воспользоваться \"Макро F1-мерой\" (деомнстрирующей среднее значение F1-меры по классам) и \"Микро F1-мерой\" (глобальная средняя F1-меры, вычисляющая сумму TP, FN и FP).\n",
    "\n",
    "Таким образом, для определения качества моделей будем использовать **f1_micro** и **f1_macro**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef910c7a",
   "metadata": {},
   "source": [
    "<a name='model'></a>\n",
    "## 3. Создание модели\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6432d",
   "metadata": {},
   "source": [
    "### 3. 1. Разделение выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2086fe8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:56:13.309588Z",
     "start_time": "2023-05-14T13:56:13.295988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Разделение датасета на тренировочную и тестовую выборки\n",
    "\n",
    "X = df_movies['subs']\n",
    "y = df_movies['level_num']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_movies['level_num']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5340e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T14:00:28.632048Z",
     "start_time": "2023-05-14T14:00:28.624171Z"
    }
   },
   "outputs": [],
   "source": [
    "# Функция для нахождения предсказаний и демонстрации метрик F1: микро и макро\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "def print_f1_score(model):\n",
    "    pipe = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    # Проверка модели на тестовой выборке\n",
    "    print(f\"{model}:\")\n",
    "    print(\"\\tf1_micro: {:.4f}\".format(f1_score(y_test, y_pred, average='micro')))\n",
    "    print(\"\\tf1_macro: {:.4f}\".format(f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df939758",
   "metadata": {},
   "source": [
    "### 3. 3. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ee4c8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:59:01.897690Z",
     "start_time": "2023-05-14T13:59:01.081933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=20, n_estimators=350, random_state=42):\n",
      "\tf1_micro: 0.5106\n",
      "\tf1_macro: 0.3754\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestClassifier(random_state=42, n_estimators=350, max_depth=20)\n",
    "print_f1_score(forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f446f7",
   "metadata": {},
   "source": [
    "### 3. 4. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15e9e1e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:59:08.217924Z",
     "start_time": "2023-05-14T13:59:07.138727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=35, max_iter=300, random_state=42):\n",
      "\tf1_micro: 0.5319\n",
      "\tf1_macro: 0.4204\n"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression(random_state=42, max_iter=300, C=35)\n",
    "print_f1_score(logreg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df1c51",
   "metadata": {},
   "source": [
    "### 3. 5. RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2acb8aa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:59:10.151024Z",
     "start_time": "2023-05-14T13:59:09.969090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier(alpha=0.1, random_state=42):\n",
      "\tf1_micro: 0.5745\n",
      "\tf1_macro: 0.4843\n"
     ]
    }
   ],
   "source": [
    "ridge_model = RidgeClassifier(random_state=42, alpha=0.1)\n",
    "print_f1_score(ridge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64c999",
   "metadata": {},
   "source": [
    "### 3. 6. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8dd4fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:59:13.176538Z",
     "start_time": "2023-05-14T13:59:12.930292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(l1_ratio=0.01, loss='huber', penalty='elasticnet',\n",
      "              random_state=42):\n",
      "\tf1_micro: 0.5745\n",
      "\tf1_macro: 0.4921\n"
     ]
    }
   ],
   "source": [
    "svc_model = SGDClassifier(random_state=42, loss='huber', alpha=0.0001, penalty='elasticnet', l1_ratio=0.01)\n",
    "print_f1_score(svc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a0637",
   "metadata": {},
   "source": [
    "### 3. 7. LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec9f82fd",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T13:59:17.261174Z",
     "start_time": "2023-05-14T13:59:17.054426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=3, random_state=42):\n",
      "\tf1_micro: 0.5745\n",
      "\tf1_macro: 0.4843\n"
     ]
    }
   ],
   "source": [
    "svc_model = LinearSVC(random_state=42, C=3)\n",
    "print_f1_score(svc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ab165",
   "metadata": {},
   "source": [
    "<a name='analysis'></a>\n",
    "## 4. Анализ результатов\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebf72",
   "metadata": {},
   "source": [
    "Из 6 рассмотренных моделей наилучший результат по метрикам 'f1_micro' и 'f1_macro' продемонстрировали модели:\n",
    "- RidgeClassifier(alpha=0.1, random_state=42):\n",
    "\t- f1_micro: 0.7857\n",
    "\t- f1_macro: 0.7684\n",
    "- LinearSVC(C=3, random_state=42):\n",
    "\t- f1_micro: 0.7857\n",
    "\t- f1_macro: 0.7684\n",
    "   \n",
    "Поскольку необходимо использовать только одну модель, возьмем вторую: **LinearSVC(C=3, random_state=42)**, и сохраним полученную модель в отдельный файл для дальнейшего внедрения модели машинного обучения в веб-приложение по определению уровню английского языка, необходимого для понимания по меньшей мере 50-70% фильма."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e8030",
   "metadata": {},
   "source": [
    "<a name='model_saving'></a>\n",
    "## 5. Сохранение модели\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1d96f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('model', svc_model)\n",
    "])\n",
    "    \n",
    "svc_pipe.fit(X_train, y_train)\n",
    "\n",
    "with open('./main.pcl', 'wb') as model_file:\n",
    "    dump(svc_pipe, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8b790761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subs</th>\n",
       "      <th>level_num_x</th>\n",
       "      <th>level_num_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>pay careful attention to everything you see an...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>saroo come on get up quickly hold it properly ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>itchy a few more degree to the left now tap no...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>you've reached doug sorry i missed your call p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and how it changed our valley forever there wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>what give you that idea want to know the secre...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hi todd mahar eharmony how can i help you toda...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>we keep moving on no messing around you mess t...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michelle please don't hang up just talk to me ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>there's a moment oforderly silence before foot...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 subs  level_num_x  \\\n",
       "48  pay careful attention to everything you see an...            4   \n",
       "50  saroo come on get up quickly hold it properly ...            3   \n",
       "5   itchy a few more degree to the left now tap no...            3   \n",
       "89  you've reached doug sorry i missed your call p...            2   \n",
       "7   and how it changed our valley forever there wa...            1   \n",
       "26  what give you that idea want to know the secre...            3   \n",
       "97  hi todd mahar eharmony how can i help you toda...            3   \n",
       "51  we keep moving on no messing around you mess t...            3   \n",
       "0   michelle please don't hang up just talk to me ...            3   \n",
       "84  there's a moment oforderly silence before foot...            1   \n",
       "\n",
       "    level_num_y  \n",
       "48            4  \n",
       "50            3  \n",
       "5             3  \n",
       "89            2  \n",
       "7             1  \n",
       "26            3  \n",
       "97            3  \n",
       "51            3  \n",
       "0             3  \n",
       "84            1  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svc_pipe.predict(X_test)\n",
    "predictions = pd.DataFrame(X_test)\n",
    "predictions = a.merge(pd.Series(y_pred).rename('level_num'), right_index=True, left_index=True)\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a64cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
